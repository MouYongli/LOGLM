{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os \n",
    "import os.path as osp\n",
    "import json\n",
    "import tiktoken\n",
    "import ollama\n",
    "from openai import AzureOpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = \"gpt-4\"\n",
    "\n",
    "project_path = '/home/mou/Projects/COLING-LogicLLM/LogLM'\n",
    "data_path = osp.join(project_path, 'data')\n",
    "demonstration_path = osp.join(project_path, 'src/logicllm/prompts/baseline')\n",
    "datasets = ['FOLIO', 'AR-LSAT', 'LogicalDeduction', 'ProntoQA', 'ProofWriter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a problem statement as contexts, the task is to answer a logical reasoning question. \n",
      "------\n",
      "Context:\n",
      "The Blake McFall Company Building is a commercial warehouse listed on the National Register of Historic Places. The Blake McFall Company Building was added to the National Register of Historic Places in 1990. The Emmet Building is a five-story building in Portland, Oregon. The Emmet Building was built in 1915. The Emmet Building is another name for the Blake McFall Company Building. John works at the Emmet Building.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? The Blake McFall Company Building is located in Portland, Oregon.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "Reasoning:\n",
      "The Blake McFall Company Building is another name for the Emmet Building. The Emmet Building is located in Portland, Oregon. Therefore, the Blake McFall Company Building is located in Portland, Oregon.\n",
      "\n",
      "The correct option is: A\n",
      "------\n",
      "Context:\n",
      "People eat meat regularly or are vegetation. If people eat meat regularly, then they enjoy eating hamburgers and steaks. All people who are vegetarian are conscious of the environment or their health. If people are conscious about the environment or their health, then they do not go to fast food places often. If people have busy schedules without time to cook, then they go to fast food places often. If Jeremy does not both go to fast food places often and is conscious about the environment or their health, then he goes to fast food places often.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? If Jeremy has a busy schedule without time to cook, then Jeremy does not enjoy eating hamburgers and steaks.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "Reasoning:\n",
      "If Jeremy has a busy schedule without time to cook or enjoy eating hamburgers and steaks, then Jeremy goes to fast food places often. If people are conscious about the environment or their health, then they do not go to fast food places often. This means that Jeremy is not conscious about the environment or his health. All people who are vegetarian are conscious of the environment or their health. Therefore, Jeremy is not vegetarian. People eat meat regularly or are vegetation. Therefore, Jeremy eats meat regularly. If people eat meat regularly, then they enjoy eating hamburgers and steaks. Therefore, Jeremy enjoys eating hamburgers and steaks. \n",
      "\n",
      "The correct option is: B\n",
      "------\n",
      "Context:\n",
      "[[CONTEXT]]\n",
      "\n",
      "Question: [[QUESTION]]\n",
      "\n",
      "Options:\n",
      "[[OPTIONS]]\n",
      "\n",
      "Reasoning:\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'FOLIO'\n",
    "split = 'dev'\n",
    "mode = 'CoT'\n",
    "stop_words = \"------\"\n",
    "label_phrase = 'The correct option is:'\n",
    "\n",
    "def prompt_creator(in_context_example, test_example):\n",
    "    full_prompt = in_context_example\n",
    "    context = test_example['context'].strip()\n",
    "    question = test_example['question'].strip()\n",
    "    options = '\\n'.join([opt.strip() for opt in test_example['options']])\n",
    "    full_prompt = full_prompt.replace('[[CONTEXT]]', context)\n",
    "    full_prompt = full_prompt.replace('[[QUESTION]]', question)\n",
    "    full_prompt = full_prompt.replace('[[OPTIONS]]', options)\n",
    "    return full_prompt\n",
    "\n",
    "with open(os.path.join(data_path, dataset_name, f'{split}.json')) as f:\n",
    "    raw_dataset = json.load(f)\n",
    "\n",
    "with open(os.path.join(demonstration_path, f'{dataset_name}_{mode}.txt')) as f:\n",
    "    in_context_examples = f.read()\n",
    "\n",
    "print(in_context_examples)\n",
    "    \n",
    "example = raw_dataset[0]\n",
    "question, answer = example['question'], example['answer']\n",
    "full_prompt = prompt_creator(in_context_examples, example)\n",
    "\n",
    "# print(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in prompt GPT-4: 700\n",
      "Bonnie either both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school. If people perform in school talent shows often, then they attend and are very engaged with school events. Therefore, if Bonnie performs in school talent shows often, then she attends and is very engaged with school events. Since Bonnie is a student who attends the school, it can be inferred that she performs in school talent shows often.\n",
      "\n",
      "The correct option is: A\n",
      "id : FOLIO_dev_0\n",
      "question : Based on the above information, is the following statement true, false, or uncertain? Bonnie performs in school talent shows often.\n",
      "answer : C\n",
      "predicted answer : A\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "print(f'Number of tokens in prompt GPT-4: {num_tokens_from_string(full_prompt, \"gpt-4\")}')\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model = deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ],\n",
    "    temperature = 0.0,\n",
    "    top_p = 1.0,\n",
    "    stop = stop_words\n",
    ")\n",
    "\n",
    "generated_content = response.choices[0].message.content.strip()\n",
    "print(generated_content)\n",
    "generated_answer = generated_content.split(label_phrase)[-1].strip()\n",
    "generated_reasoning = generated_content.split(label_phrase)[0].strip()\n",
    "\n",
    "\n",
    "output_json = {'id': example['id'], \n",
    "            'question': question, \n",
    "            'answer': answer, \n",
    "            'predicted_reasoning': generated_reasoning,\n",
    "            'predicted_answer': generated_answer}\n",
    "print(f'id : {example[\"id\"]}')\n",
    "print(f'question : {question}')\n",
    "print(f'answer : {answer}')\n",
    "# print(f'predicted reasoning : {generated_reasoning}')\n",
    "print(f'predicted answer : {generated_answer}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bonnie either both attends and is very engaged with school events and is a '\n",
      " 'student who attends the school, or she neither attends and is very engaged '\n",
      " 'with school events nor is a student who attends the school. If people '\n",
      " 'perform in school talent shows often, then they attend and are very engaged '\n",
      " 'with school events. Therefore, if Bonnie performs in school talent shows '\n",
      " 'often, then she attends and is very engaged with school events. Since Bonnie '\n",
      " 'is a student who attends the school, it can be inferred that she performs in '\n",
      " 'school talent shows often.\\n'\n",
      " '\\n'\n",
      " 'The correct option is: A')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(response.choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in prompt Llama 3.1-8B: 701\n",
      "Let's analyze the context and question step by step:\n",
      "\n",
      "1. If people perform in school talent shows often, then they attend and are very engaged with school events.\n",
      "2. People either perform in school talent shows often or are inactive and disinterested members of their community.\n",
      "3. All people who are inactive and disinterested members of their community chaperone high school dances.\n",
      "4. Bonnie either both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school.\n",
      "\n",
      "From statement 2, we know that Bonnie either performs in school talent shows often or is an inactive and disinterested member of her community. \n",
      "\n",
      "If Bonnie is an inactive and disinterested member of her community (which means she chaperones high school dances), then it would be incorrect to say that she performs in school talent shows often.\n",
      "\n",
      "However, the question asks if Bonnie performs in school talent shows often, which implies that we should consider the first option: \"Bonnie either both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school.\"\n",
      "\n",
      "Since Bonnie being an inactive and disinterested member of her community would mean she does not perform in school talent shows often (because that's what statement 2 says), we can conclude that if Bonnie performs in school talent shows often, it must be because she attends and is very engaged with school events.\n",
      "\n",
      "Therefore, the correct answer is:\n",
      "\n",
      "A) True\n",
      "\n",
      "This reasoning assumes that Bonnie being a student who attends the school implies that she performs in school talent shows often (because of statement 1).\n"
     ]
    }
   ],
   "source": [
    "llama31_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "tokens = llama31_tokenizer.encode(full_prompt)\n",
    "print(f'Number of tokens in prompt Llama 3.1-8B: {len(tokens)}')\n",
    "\n",
    "# stream = ollama.chat(\n",
    "#     model='llama3.1',\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": full_prompt}\n",
    "#     ],\n",
    "#     stream=True,\n",
    "# )\n",
    "# for chunk in stream:\n",
    "#   print(chunk['message']['content'], end='', flush=True)\n",
    "stream = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ],\n",
    "    stream=False,\n",
    "    options={\n",
    "        'temperature': 0.0,\n",
    "        # 'seed': 47,\n",
    "    }\n",
    ")\n",
    "print(stream['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
