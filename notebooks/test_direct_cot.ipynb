{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os \n",
    "import os.path as osp\n",
    "import json\n",
    "import tiktoken\n",
    "import ollama\n",
    "from openai import AzureOpenAI\n",
    "from transformers import AutoTokenizer\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = \"gpt-4\"\n",
    "\n",
    "data_path = '/home/mou/Projects/COLING-LogicLLM/LOGLM/data'\n",
    "demonstration_path = '/home/mou/Projects/COLING-LogicLLM/LOGLM/src/logicllm/prompts/baseline'\n",
    "datasets = ['FOLIO', 'AR-LSAT', 'LogicalDeduction', 'ProntoQA', 'ProofWriter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a problem statement as contexts, the task is to answer a logical reasoning question. \n",
      "------\n",
      "Context:\n",
      "All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "The correct option is: A\n",
      "------\n",
      "Context:\n",
      "William Dickinson was a British politician who sat in the House of Commons William Dickinson attended Westminster school for high school and then the University of Edinburgh. The University of Edinburgh is a university located in the United Kingdom. William Dickinson supported the Portland Whigs. People who supported the Portland Whigs did not get a seat in the Parliament.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? William Dickinson went to schools located in the United Kingdom for both high school and university.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "The correct option is: C\n",
      "------\n",
      "Context:\n",
      "If people perform in school talent shows often, then they attend and are very engaged with school events. People either perform in school talent shows often or are inactive and disinterested members of their community. If people chaperone high school dances, then they are not students who attend the school. All people who are inactive and disinterested members of their community chaperone high school dances. All young children and teenagers who wish to further their academic careers and educational opportunities are students who attend the school. Bonnie either both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? Bonnie performs in school talent shows often.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "The correct option is:\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'FOLIO'\n",
    "split = 'dev'\n",
    "mode = 'Direct'\n",
    "stop_words = \"------\"\n",
    "label_phrase = 'The correct option is:'\n",
    "\n",
    "def prompt_creator(in_context_example, test_example):\n",
    "    full_prompt = in_context_example\n",
    "    context = test_example['context'].strip()\n",
    "    question = test_example['question'].strip()\n",
    "    options = '\\n'.join([opt.strip() for opt in test_example['options']])\n",
    "    full_prompt = full_prompt.replace('[[CONTEXT]]', context)\n",
    "    full_prompt = full_prompt.replace('[[QUESTION]]', question)\n",
    "    full_prompt = full_prompt.replace('[[OPTIONS]]', options)\n",
    "    return full_prompt\n",
    "\n",
    "with open(os.path.join(data_path, dataset_name, f'{split}.json')) as f:\n",
    "    raw_dataset = json.load(f)\n",
    "\n",
    "with open(os.path.join(demonstration_path, f'{dataset_name}_{mode}.txt')) as f:\n",
    "    in_context_examples = f.read()\n",
    "    \n",
    "example = raw_dataset[0]\n",
    "question, answer = example['question'], example['answer']\n",
    "full_prompt = prompt_creator(in_context_examples, example)\n",
    "\n",
    "print(full_prompt)\n",
    "\n",
    "# batch_size = 10\n",
    "# dataset_chunks = [raw_dataset[i:i + batch_size] for i in range(0, len(raw_dataset), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in prompt GPT-4: 503\n",
      "A) True\n",
      "id : FOLIO_dev_0\n",
      "question : Based on the above information, is the following statement true, false, or uncertain? Bonnie performs in school talent shows often.\n",
      "answer : C\n",
      "predicted answer : A) True\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "print(f'Number of tokens in prompt GPT-4: {num_tokens_from_string(full_prompt, \"gpt-4\")}')\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,  \n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model = deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ],\n",
    "    temperature = 0.0,\n",
    "    top_p = 1.0,\n",
    "    stop = stop_words\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "generated_content = response.choices[0].message.content.strip()\n",
    "print(generated_content)\n",
    "generated_answer = generated_content.split(label_phrase)[-1].strip()\n",
    "generated_reasoning = generated_content.split(label_phrase)[0].strip()\n",
    "\n",
    "\n",
    "output_json = {'id': example['id'], \n",
    "            'question': question, \n",
    "            'answer': answer, \n",
    "            'predicted_reasoning': generated_reasoning,\n",
    "            'predicted_answer': generated_answer}\n",
    "print(f'id : {example[\"id\"]}')\n",
    "print(f'question : {question}')\n",
    "print(f'answer : {answer}')\n",
    "# print(f'predicted reasoning : {generated_reasoning}')\n",
    "print(f'predicted answer : {generated_answer}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in prompt Llama 3.1-8B: 504\n",
      "It seems like you're providing a series of context and questions. I'll answer each question based on the given information.\n",
      "\n",
      "For the first question:\n",
      "\n",
      "Context:\n",
      "All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "Answer: A) True\n",
      "\n",
      "Explanation: According to the context, people either regularly drink coffee (which implies they are dependent on caffeine) or joke about being addicted to caffeine. Rina's situation is not explicitly mentioned as one of these two options, but it can be inferred that she must fit into one of them. Therefore, the statement \"Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug\" is true.\n",
      "\n",
      "For the second question:\n",
      "\n",
      "Context:\n",
      "William Dickinson was a British politician who sat in the House of Commons William Dickinson attended Westminster school for high school and then the University of Edinburgh. The University of Edinburgh is a university located in the United Kingdom. William Dickinson supported the Portland Whigs. People who supported the Portland Whigs did not get a seat in the Parliament.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? William Dickinson went to schools located in the United Kingdom for both high school and university.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "Answer: C) Uncertain\n",
      "\n",
      "Explanation: The context states that William Dickinson attended Westminster school (which is not explicitly mentioned as being located in the UK) for high school, and then the University of Edinburgh (which is indeed located in the UK). However, it does not provide information about whether Westminster school is also located in the UK. Therefore, the statement \"William Dickinson went to schools located in the United Kingdom for both high school and university\" is uncertain.\n",
      "\n",
      "For the third question:\n",
      "\n",
      "Context:\n",
      "If people perform in school talent shows often, then they attend and are very engaged with school events. People either perform in school talent shows often or are inactive and disinterested members of their community. If people chaperone high school dances, then they are not students who attend the school. All people who are inactive and disinterested members of their community chaperone high school dances. All young children and teenagers who wish to further their academic careers and educational opportunities are students who attend the school. Bonnie either both attends and is very engaged with school events and is a student who attends the school, or she neither attends and is very engaged with school events nor is a student who attends the school.\n",
      "\n",
      "Question: Based on the above information, is the following statement true, false, or uncertain? Bonnie performs in school talent shows often.\n",
      "\n",
      "Options:\n",
      "A) True\n",
      "B) False\n",
      "C) Uncertain\n",
      "\n",
      "Answer: C) Uncertain\n",
      "\n",
      "Explanation: The context states that people either perform in school talent shows often or are inactive and disinterested members of their community. However, it does not provide information about Bonnie's specific situation regarding performing in school talent shows. Therefore, the statement \"Bonnie performs in school talent shows often\" is uncertain.\n",
      "\n",
      "Please let me know if you'd like me to answer any further questions!\n"
     ]
    }
   ],
   "source": [
    "llama31_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "tokens = llama31_tokenizer.encode(full_prompt)\n",
    "print(f'Number of tokens in prompt Llama 3.1-8B: {len(tokens)}')\n",
    "\n",
    "# stream = ollama.chat(\n",
    "#     model='llama3.1',\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": full_prompt}\n",
    "#     ],\n",
    "#     stream=True,\n",
    "# )\n",
    "# for chunk in stream:\n",
    "#   print(chunk['message']['content'], end='', flush=True)\n",
    "stream = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ],\n",
    "    stream=False,\n",
    "    options={\n",
    "        'temperature': 0.0,\n",
    "        'seed': 47,\n",
    "    }\n",
    ")\n",
    "print(stream['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
